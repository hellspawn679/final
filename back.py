from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel
import google.generativeai as genai
import os 
from vector_store import VectorStore
import numpy as np
import spacy
import re
os.environ["GOOGLE_API_KEY"] = "AIzaSyCnfxeSjZ4YzCUM8u5jNsdrJGcmhBIwIZo"
genai.configure(api_key="AIzaSyCnfxeSjZ4YzCUM8u5jNsdrJGcmhBIwIZo")
model=genai.GenerativeModel(model_name="gemini-pro")
doc=[]
vector_index = None
app = FastAPI()
chat =None
# Create a VectorStore instance
vector_store = VectorStore()
def vector_embb(texts):
    nlp = spacy.load("en_core_web_md")
    docs = [nlp(text) for text in texts]
    # Accessing the vector for the first token in the first document
    return docs
def vector_embb_query(texts):
   nlp = spacy.load("en_core_web_md")
   return nlp(texts)
def divide_in_chuncks(text, chunk_size):
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
def mapping(tokens):
    """
    Maps unique tokens to their corresponding IDs and vice versa.

    Args:
        tokens (list): A list of tokens.

    Returns:
        tuple: A tuple containing two dictionaries:
            - word_to_id: A dictionary mapping tokens to their corresponding IDs.
            - id_to_word: A dictionary mapping IDs to their corresponding tokens.
    """
    word_to_id = {}
    id_to_word = {}
    for i, token in enumerate(set(tokens)):
        word_to_id[token] = i
        id_to_word[i] = token
    
    return word_to_id, id_to_word
def preprocessing(paragraph):
    text = re.sub(r'\s+',' ',paragraph)
    text = text.lower()
    text = re.sub(r'\s+',' ',text)
    return text
class Item(BaseModel):
    filename: str
    filetype: str
    filesize: int 
    data: str
class question(BaseModel):
    querie:str
@app.post("/qna/")
async def upload_file(item: question): 
    """
    Uploads a file and performs a question-answering task.

    Args:
        item (question): The question item containing the query sentence.

    Returns:
        dict: A dictionary containing the response generated by the question-answering model.
    """
    global vector_store
    global docs
    top_results=3
    query_sentence = item.querie
    vec=vector_embb_query(query_sentence).vector

    similar_sentences = vector_store.find_similar_vectors(vec, num_results=top_results)
    # Print similar sentences
    print("Query Sentence:", query_sentence)
    print("Similar Sentences:")
    indexe=[]
    for sentence, similarity in similar_sentences:
        print(f"{sentence}: Similarity = {similarity:.4f}")
        indexe.append(sentence)
    chat = model.start_chat(history=[])
    for i in range(top_results):
        prompt="Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.and don't start to generate answer till i start the prompt with 'answer it'{}".format(docs[indexe[i]])
        chat.send_message(prompt)
    response=chat.send_message("answer it"+item.querie)
    return {"response": response.text}
@app.post("/upload/")
async def upload_file(item: Item):
    global vector_store
    global docs
    x= item.data.lower()
    texts=divide_in_chuncks(x, 1000)
    docs=vector_embb(texts)
    index= {}
    for i in range(len(docs)):
         index[i]=docs[i].vector
    for sentence, vector in index.items():
         vector_store.add_vector(sentence, vector)
    
    return {"response": "done"}
@app.get("/")
async def root():
    return {"message": "Hello World"}
#uvicorn back:app --reload  
#docker compose up -d --build  --scale server=1      
 